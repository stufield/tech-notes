---
title: "Technical note: na&iuml;ve Bayes Classifiers"
author: "Stu Field"
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_document:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
source("robustNaiveBayes.R")
library(ggplot2)
knitr::opts_chunk$set(
 collapse = TRUE,
 comment  = "#>"
)

thm <- theme_bw() + 
  theme(
    panel.background  = element_rect(fill = "transparent", colour = NA), 
    plot.background   = element_rect(fill = "transparent", colour = NA),
    legend.position   = "right",
    panel.border      = element_blank(),
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key        = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)

burst_model_object <- function(x) {
  for (f in names(x$tables)) {
    tab <- x$tables[[f]]
    assign(paste0(f, "_control_mu"), tab[["control", 1L]], envir = .GlobalEnv)
    assign(paste0(f, "_control_sd"), tab[["control", 2L]], envir = .GlobalEnv)
    assign(paste0(f, "_disease_mu"), tab[["disease", 1L]], envir = .GlobalEnv)
    assign(paste0(f, "_disease_sd"), tab[["disease", 2L]], envir = .GlobalEnv)
  }
}

train <- readRDS("bayes-data.rds")
bayes_model <- robustNaiveBayes(Response ~ ., data = train)
burst_model_object(bayes_model)
counts <- bayes_model$apriori
prior  <- prop.table(counts)
sample_data <- train[c(3, 4, 5, 257, 267), 1:2]
s1  <- as.numeric(sample_data[1L, ])
s2  <- as.numeric(sample_data[2L, ])
s3  <- as.numeric(sample_data[3L, ])
s4  <- as.numeric(sample_data[4L, ])
s5  <- as.numeric(sample_data[5L, ])
s1f1 <- s1[1L]  # sample 1; Feature 1
s1f2 <- s1[2L]  # sample 1; Feature 2
d11 <- dnorm(s1f1, feat1_disease_mu, feat1_disease_sd)  # class 2; feature 1
d12 <- dnorm(s1f2, feat2_disease_mu, feat2_disease_sd)  # class 2; feature 2
d21 <- dnorm(s1f1, feat1_control_mu, feat1_control_sd)  # class 1; feature 1``
d22 <- dnorm(s1f2, feat2_control_mu, feat2_control_sd)  # class 1; feature 2
p_cont <- d21 * d22 * prior[1L]
p_case <- d11 * d12 * prior[2L]
```


----------


# Overview

The general goal is to use a probabilistic modeling framework to predict
the class of an unknown sample. Such mathematical models, or classifiers,
are based on training data and are built (i.e. model fitting) in order
to make class predictions about the unknown sample.


--------


## Bayes' Theorem
$$
\begin{equation}
  Posterior\ Probability = \frac{Likelihood(data) \times Prior}{Evidence}
\end{equation}
$$

written in terms of probabilities,

$$
\begin{equation}
  \tag{1}
  P(outcome\ |\ data) = \frac{P(data\ |\ outcome) \times P(outcome)}{P(data)}
\end{equation}
$$

where the term $P(data)$ is a normalizing constant that is independent of the
*outcome*, and is often ignored if *relative* posteriors are desired
over *absolute* posteriors. Equation (tag:1) then simplifies to,

$$
\begin{equation}
  P(outcome\ |\ data) \propto P(data\ |\ outcome) \times P(outcome)
\end{equation}
$$


## Na&iuml;ve Bayes in Practice

Consider an example where *proteomic* measurements (i.e. features = proteins),
assumed to have Gaussian (i.e. Normal) distributions, are taken from an
individual and the posterior probability of interest is whether that
individual belongs to one of $k$ possible outcomes/classes.
For example, if there are $k=2$ possible classes,
disease or control (i.e. a *binary* classifier), this can be re-written as:

$$
\begin{eqnarray}
   P(control\ |\ protein\ conc) &=& P(protein\ conc\ |\ control) \times P(control), \\
   P(disease\ |\ protein\ conc) &=& P(protein\ conc\ |\ disease) \times P(disease).
\end{eqnarray}
$$

If we have $p$ features and na&iuml;vely assume all are independent, we can
multiply their individual probabilities to produce a cumulative probability.
For the disease posterior this gives:

$$
\begin{eqnarray}
   P(control\ |\ protein\ conc) &=& P(control) \times P(prot_1\ |\ control) \times P(prot_2\ |\ control) \times ... \times P(prot_p\ |\ control), \\
   P(disease\ |\ protein\ conc) &=& P(disease) \times P(prot_1\ |\ disease) \times P(prot_2\ |\ disease) \times ... \times P(prot_p\ |\ disease).
\end{eqnarray}
$$

Na&iuml;ve Bayes models contain $2pk + k$ parameters, where $k$ is the number
of classes, $p$ is the number of features; a **mean** ($\mu$) and
**standard deviation** ($\sigma$) for each feature $\times$ class combination,
plus the class-specific **prior**, which is often determined by the training
class proportions (i.e. an *uninformative* prior).
Na&iuml;ve Bayes models assume Gaussian densities and are calculated via the
probability density function (PDF) given class-specific parameters
$\mu$ and $\sigma$:

$$
\begin{eqnarray}
  \tag{pdf}
   f(x\ |\ \mu_k,\sigma_k) &= \frac{1}{\sqrt{2\pi\sigma_k^2}} \; exp\Bigg( \frac{-(x-\mu_k)^2}{2\sigma_k^2} \Bigg),
\end{eqnarray}
$$

To classify an unknown sample with $p$ feature measurements
($\vec x = x_1,...,x_p$) and $k$ classes, calculate the following:

$$
\begin{eqnarray} \tag{bayes}
   P(k\ |\ \vec x) &= 
      \Bigg[ \prod_{i=1}^{p} \frac{1}{\sqrt{2\pi\sigma_{ik}^2}} \; exp\Bigg( \frac{-(x_i-\mu_{ik})^2}{2\sigma_{ik}^2} \Bigg) \Bigg] \times P(k),
\end{eqnarray}
$$

The result of Equation (tag:bayes) gives a probability *density* for
each class, which is not constrained on the interval $[0,\ 1]$.
Normalized posterior probabilities ($Pr$) are obtained by calculating the
class-specific proportion of the total density,

$$
\begin{eqnarray} 
  \tag{bayes_prob}
  Pr(k=j\ |\ \vec x) &=& \frac{P(j\ |\ \vec x)}{\sum_{i=1}^k P(i\ |\ \vec x)}
\end{eqnarray}
$$



## Example Calculation

### Training data

```{r train}
tibble::as_tibble(train)
```


### The Model

Consider a $k = 2$ class (disease vs. control) and $p = 2$ feature
example, with $2pk + k = 10$ parameters. The na&iuml;ve Bayes model looks
like this:

```{r bayes-model}
bayes_model
```

With specific parameters:

| **Model Parameters**                          | **Control**          | **Disease**       |
| :-------------------------------------------- | :------------------- | :---------------- |
| Training samples                              | `r counts[1L]`       | `r counts[2L]`    |
| Prevalence (prior)                            | `r prior[1L]`        | `r prior[2L]`     |
| Feature 1 ($\hat{\mu}_c,\ \hat{\mu}_d$)       | `r feat1_control_mu` | `r feat1_disease_mu` |
| Feature 1 ($\hat{\sigma}_c,\ \hat{\sigma}_d$) | `r feat1_control_sd` | `r feat1_disease_sd` |
| Feature 2 ($\hat{\mu}_c,\ \hat{\mu}_d$)       | `r feat2_control_mu` | `r feat2_disease_mu` |
| Feature 2 ($\hat{\sigma}_c,\ \hat{\sigma}_d$) | `r feat2_control_sd` | `r feat2_disease_sd` |


#### Sample "new" data
5 unknown samples with 2 measurements each:

```{r sample-data}
tibble::as_tibble(sample_data)
```


#### Raw calculation sample 1:

For unknown **sample 1**, the na&iuml;ve Bayes posterior conditional
probability densities are calculated using equation~\eqref{eqn:bayes}
as follows:

**Control posterior density:**
$$
\begin{eqnarray*}
   P(control\ |\ x_1=`r s1f1`,x_2=`r s1f2`) &=& P(x_1=`r s1f1`\ |\ control) \times \\
               && P(x_2=`r s1f2`\ |\ control) \times \\
               && P(control) \\
               &=& P(x_1=`r s1f1`\ |\ \mu_c=`r feat1_control_mu`, \sigma_c=`r feat1_control_sd`) \times \\
               && P(x_2=`r s1f2`\ |\ \mu_c=`r feat2_control_mu`, \sigma_c=`r feat2_control_sd`) \times \\
               && (`r counts[1]`\ /\ (`r counts[1L]` + `r counts[2L]`)) \\
               &=& `r d21` \times `r d22` \times `r prior[1L]` \\
               &=& `r d21 * d22 * prior[1L]` \\
\end{eqnarray*}
$$


**Disease posterior density:**
$$
\begin{eqnarray*}
   P(disease\ |\ x_1=`r s1f1`,x_2=`r s1f2`) &=& P(x_1=`r s1f1`\ |\ disease) \times \\ 
              && P(x_2=`r s1f2`\ |\ disease) \times \\
              && P(disease) \\
              &=& P(x_1=`r s1f1`\ |\ \mu_d=`r feat1_disease_mu`, \sigma_d=`r feat1_disease_sd`) \times \\
              && P(x_2=`r s1f2`\ |\ \mu_d=`r feat2_disease_mu`, \sigma_d=`r feat2_disease_sd`) \times \\
              && (`r counts[2L]`\ /\ (`r counts[1L]` + `r counts[2L]`)) \\
              &=& `r d11` \times `r d12` \times `r prior[2L]` \\
              &=& `r d11 * d12 * prior[2L]` \\
\end{eqnarray*}
$$


**Normalized Posterior Probabilities:**

From Equation (tag:bayes_prob), the relative proportion of each density is:

$$
\begin{eqnarray*}
   Pr(control\ |\ \vec x) &=&
      \frac{ `r p_cont` }{ `r p_case` + `r p_cont` } = `r p_cont / (p_case + p_cont)` \\
   && \\
   Pr(disease\ |\ \vec x) &=&
      \frac{ `r p_case` }{ `r p_case` + `r p_cont` } = `r p_case / (p_case + p_cont)` \\
\end{eqnarray*}
$$

Normalized posterior probabilities for 5 unknown samples are shown below.
The disease class prediction is based on a decision cutoff of
$Pr(disease) \ge 0.5$

```{r preds}
cutoff <- 0.5
preds  <- tibble::as_tibble(predict(bayes_model, sample_data, type = "raw"))
preds$class  <- ifelse(preds$disease >= cutoff, "disease", "control")
names(preds) <- c("Pr(control)", "Pr(disease)", "Class Prediction")
preds
```


-------




## Na&iuml;ve Bayes Visualization

Probability density functions (scaled by sample size) of the training data
used to fit the na&iuml;ve Bayes model.
Curves are colored by class and the feature measurements for Sample 1
($\vec x =$ `r s1f1`, `r s1f2`) are represented by the dashed vertical line.
This graphically indicates that Sample 1 is more likely to have come from
the control distribution.

```{r PDFs, fig.width = 10, fig.height = 4}
p1 <- train |>
  dplyr::select(feat1, Response) |>
  ggplot(aes(x = feat1, fill = Response)) +
  geom_density(alpha = 0.25, size = 0.1) +
  labs(y = "Probability Density",  title = "Feature 1", x = "value") +
  geom_vline(xintercept = s1f1, linetype = "dashed")
p2 <- train |>
  dplyr::select(feat2, Response) |>
  ggplot(aes(x = feat2, fill = Response)) +
  geom_density(alpha = 0.25, size = 0.1) +
  labs(y = "Probability Density",  title = "Feature 2", x = "value") +
  geom_vline(xintercept = s1f2, linetype = "dashed")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```



## Bivariate Plots and Decision Boundary

```{r train-bivariate}
p1 <- ggplot(train, aes(x = feat1, y = feat2)) + 
  geom_point(aes(fill = Response), alpha = 0.5, size = 3,
             stroke = 1, shape = 21) +
  geom_vline(xintercept = c(feat1_control_mu, feat1_disease_mu),
             color = c("#F8766D", "#00BFC4"), linetype = "dashed") +
  geom_hline(yintercept = c(feat2_control_mu, feat2_disease_mu),
             color = c("#F8766D", "#00BFC4"), linetype = "dashed") +
  geom_rug(colour = "navy", size = 0.25, length = unit(0.01, "npc")) +
  labs(title = "Training Data")
```


```{r boundary-fun}
plotBayesBoundary <- function(data, pos.class, res = 50, main = NULL) {
  stopifnot(ncol(data) == 3, is.numeric(res))
  train <- data |>
    dplyr::rename_if(is.factor, function(.x) "class") |> # rename response
    dplyr::rename_at(1:2, function(.x) c("F1", "F2"))    # rename features 1,2
  train$class <- factor(train$class,    # pos.class 2nd
                        levels = c(setdiff(train$class, pos.class), pos.class))
  model <- robustNaiveBayes(class ~ F1 + F2, data = train)
  df <- purrr::cross_df(
     list(F1 = seq(min(train$F1), max(train$F1), length = res),
          F2 = seq(min(train$F2), max(train$F2), length = res)))
  df$Pr <- predict(model, newdata = df, type = "raw")[, 2L]

  p <- ggplot(df, aes(x = F1, y = F2))
  p + geom_raster(aes(fill = Pr), alpha = 0.5) +
    scale_fill_gradient(high = "#00BFC4", low = "#F8766D") +
    geom_contour(aes(x = F1, y = F2, z = Pr), binwidth = 0.5,
                 color = "darkorchid4", linetype = "dashed") +
    geom_point(data = train, mapping = aes(x = F1, y = F2, color = class),
               size = 2.5, alpha = 0.5) +
    geom_point(data = train, aes(x = F1, y = F2),
               size = 2.5, shape = 21, color = "black") +
    labs(x = "Feature 1", y = "Feature 2", title = main)
}
p2 <- plotBayesBoundary(
  train, pos.class = "disease", main = "Bayes Decision Boundary"
  ) +
  geom_point(data = sample_data, aes(x = feat1, y = feat2),
             shape = "cross", color = "green", size = 3, stroke = 2)
```


Bivariate plots of training data used to fit the two feature
na&iuml;ve Bayes model. Dotted lines are the class specific means of the
model parameters, points are colored by class.

The non-linear Bayes decision boundary reflecting
the $p = 0.5$ cutoff is represented by the "purple" dashed line.
The green `X`'s represent the bivariate coordinates of samples 1--5.

```{r bivariate-decision-boundary, fig.width = 12, fig.height = 5, echo = FALSE}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```


------

Created by [Rmarkdown](https://github.com/rstudio/rmarkdown)
(v`r utils::packageVersion("rmarkdown")`) and `r R.version$version.string`.
