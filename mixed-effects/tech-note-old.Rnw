\documentclass[11pt]{article}
\input{/Users/sfield/texmf/stuTeX}
\geometry{left=0.75in, right=0.75in, bottom=1in, top=1.0in, foot=0.5in, head=1in}
\parindent = 0.5cm

\pagestyle{fancy}
\rhead{\leftmark}
\lhead{Confidential}
%\lhead{\rightmark}

<<setup_opts, include=FALSE, tidy=FALSE>>=
opts_chunk$set(
  fig.path   = "plots/mixed-fig_", # path for any plot figures to be saved
  fig.keep   = "high",             # how many figures to keep
  prompt     = TRUE,               # symbol for the continuation prompt
  tidy       = FALSE,              # should output be cleaned up?
  fig.align  = "center",           # alignment for figures
  fig.show   = "hold",             # should all plots be evaluated
  dev        = "pdf",              # which output file type for figures
  comment    = NA,                 # symbol for R results output
  cache      = FALSE,              # should the chunk be cached?
  cache.path = "cache/",           # the path for the cached chunks 
  include    = TRUE,               # should the chunk be included as output?
  echo       = TRUE,               # should the chunk be echoed to the pdf?
  results    = "markup",           # how should results look: markup, tex, verbatim
  warning    = FALSE,              # should warnings be printed?
  message    = FALSE,              # should messages be printed?
  #out.width = "0.95\\textwidth",  # overall width of the figure output to file
  fig.height = 10,                 # figure height
  fig.width  = 10,                 # figure width
  highlight  = TRUE,               # should function calls be highlighted if echo=T?
  background = "gray90")           # background color for echoed chunks/results
options(replace.assign = TRUE, digits = 5, width = 85)

## For the conversion to word document with pandoc, we need file extension of figure files
## Ref: http://stackoverflow.com/questions/11915328/make-sweave-or-knitr-put-graphics-suffix-in-includegraphics
knit_hooks$set(plot = function(x, options) {
  # x <- paste(x, collapse = '.')  # x is file.ext now instead of c(file, ext)
  x <- paste(x, "dummy", collapse=".", sep=".")
  paste0("\\end{kframe}", hook_plot_tex(x, options), "\\begin{kframe}")
})
library(ggplot2)
library(MASS)
library(SomaMixedEffects)
y_lab <- bquote("Response ("*y[ij]*")")
@



\hypersetup{
  pdftitle = {Tech Note: Mixed-Effects Models in SOMAmer data},    % title
  colorlinks = true,            % color all links
  linkcolor = blue,             % crossrefs
  citecolor = blue,             % citations
  pdfauthor = {Stu Field},      % authors
  pdfkeywords = {SomaLogic, SOMAmer, Biomarker, Machine Learning, Linear Mixed-Effects Models},
  pdfsubject = {Linear Mixed-effects Model},     % description
  pdfpagemode = UseNone
}


\title{Technical Note: Primer on Longitudinal Data Analysis\\via Linear Mixed-Effects Models}
\author{Stu Field, SomaLogic, Inc.}
\date{} 



\begin{document}
\maketitle
% Timestamp
\begin{textblock*}{5cm}(12cm,-6.0cm)
  \fbox{\footnotesize \textbf{TimeStamp:} \timestamp}
\end{textblock*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Linear Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
For a simple linear-\emph{fixed} effect model, the 
model equation fits the equation:

\begin{eqnarray} \label{eqn:lm}
   y_i &\sim& \beta_0 + \beta_1 x_i + \epsilon_i \\
  \epsilon &\sim& N(0,\sigma^2_n), \nonumber
\end{eqnarray}
%
for the $i^{th}$ sample/observation. Thus, we can estimate
the model coefficients via,
%
\begin{equation} 
   \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i 
\end{equation}
%
where,
%
\begin{eqnarray*}
   \hat{\beta}_1 &=& \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
   \hat{\beta}_0 &=& \bar{y} - \hat{\beta}_1 \bar{x} \\
   \epsilon_i &=& r_i  \\
  &=& y_i - \hat{y}_i \\
  RSS &=& \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
  &=& (y_1 - \hat{\beta_0} - \hat{\beta_1}x_1)^2 +
       (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2)^2 + \dots +
       (y_n - \hat{\beta_0} - \hat{\beta_1}x_n)^2 \\
   \text{and} \\
  R^2 &=& (TSS - RSS) / TSS \\
  TSS &=& \sum (y_i - \bar{y})^2
\end{eqnarray*}

The Total Sum of Squares (TSS) measures the total variance in the
response ($Y$), thus $R^2$ is the proportion of the total variance
in $Y$ that can be explained by the model. The remaining variance is
packed into $\epsilon$. To calculate the accuracy of the coefficient
estimates, we need a measure of variance in $Y$:

\begin{eqnarray*}
   \text{SE}(\hat\beta_0)^2 &=& \sigma^2 \bigg[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \bigg] \\
     && \\
   \text{SE}(\hat\beta_1)^2 &=& \frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \\
   \text{where,} && \\
   \sigma^2 &=& Var(\epsilon), \\
\end{eqnarray*}
%
and $\sigma^2$ can be estimated from the data via 
the Residual Standard Error (RSE):
%
\begin{equation*}
  RSE = \sqrt{RSS/(n-2)}
\end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple-Linear Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The linear model can easily be expanded to include multiple predictors
(regressors/variables/proteins). The extension of Eqn~\eqref{eqn:lm}
can be modeled as:
%
\begin{eqnarray}
   y_i &=& \beta_0 x_{0i} + \beta_1 x_{1i} + \dots + \beta_p x_{pi} + \epsilon_i \\
   \nonumber
   \epsilon_i &\sim& \text{NID}(0,\sigma^2)
\end{eqnarray}
%
for the $i^{th}$ sample and $p^{th}$ covariate. We typically set $x_{0i}=1$
so that $\beta_0$ is a constant intercept. In this form, the only
\emph{random-effect} is the error term, $\epsilon_i$. This equation can be
rewritten in matrix form as:


\begin{eqnarray}
   \begin{pmatrix}
     y_1 \\ y_2 \\ \vdots \\ y_n
  \end{pmatrix}
     &=&
  \begin{pmatrix}
    x_{11} & x_{12} & \dots & x_{1p} \\
    x_{21} & x_{22} & \dots & x_{2p} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & \dots & x_{np} \\
  \end{pmatrix}
  *
  \begin{pmatrix}
   \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
  \end{pmatrix}
  +
  \begin{pmatrix}
    \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
  \end{pmatrix}
  \\ \nonumber && \\ \nonumber
  \vec y &=& \bm{X} \vec \beta + \vec \epsilon
\end{eqnarray}
%
where $\vec y$ is a $n \times 1$ vector, $\bm{X}$ is the
$n \times p$ data matrix, $\vec \beta$ is a $p \times 1$
vector of fixed effects coefficients for the $p$
covariates, and $\vec \epsilon$ is the $n\times1$ vector
of observation specific errors.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Mixed-Effect Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Mixed-effects models fit both fixed effects \emph{and} random effects in the
same model. We assume a grouping or dependence structure (typically 
temporal or spatial) that is sampled from a larger population (e.g. plots
within a farm, schools within districts, or observations within subjects).
We also assume \emph{compound symmetry}, which is a fancy way of saying
that the related samples vary in the same fashion for each ''group`` (subject),
i.e. the off-diagonal of the variance-covariance matrix are all equal.
For longitudinal data, samples from the same individual reflect serial
dependence and the model must account for this non-independence in sample
structure. Typically we are interested in fitting subject-specific (random)
intercepts (i.e. parallel linear fits can hit the y-axis independently)
and define the following:
%
\begin{eqnarray*}
   y_{ij} &=& b_{0i} + \beta_1 x_{ij} + \epsilon_{ij} \\
  b_i &\sim& N(0,\sigma^2_b) \\
  \epsilon &\sim& N(0,\sigma^2_n)
\end{eqnarray*}
%
for the $j^{th}$ observation of the $i^{th}$ subject, where $b$ is a
parameter treated as a random variable (i.e. $i$ subjects sampled
from a large, infinite population of potential subjects).
These random effects are thus assumed to vary by group (in this
case subjects) \dots the goal is to capture this variation in the
coefficients and prevent it from being packaged into $\epsilon$!



%%%%%%%%%%%%%%%%%%%%%%%
\section{Syntax in R}
%%%%%%%%%%%%%%%%%%%%%%%
Use the \code{nlme} or \code{lme4} package. {\sf R} uses the
Wilkinson notation in specifying model formulas.

\subsection{Random Intercept}
\code{lme(y $\sim$ time * group, random = $\sim$1 | pid, data = adat)}

\subsection{Random Slope}
\code{lme(y $\sim$ time * group, random = $\sim$time | pid, data = adat)}

\subsection{Random Intercept \& Random Slope}
\code{lme(y $\sim$ time * group, random = $\sim$1 + time | pid, data = adat)}

\subsection{Syntax Definitions}
\begin{itemize}
    \item \code{y} $=y_{ij}$
    \item $\sim1=$ random effects ($b_{0i}$)
    \item \code{pid} = field containing the dependent groups (subjects)
    \item \code{time * group} = the fixed effects with interaction term
    \item \textbf{note:} \code{time * group} = \code{time + group + time*group}
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Example 1: Response to Drug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
First, simulate longitudinal data for 20 subjects, having $3-10$ serial
samples each, intercept ($\beta_0=1000$), slope ($\beta_1=400$), and a
serial autocorrelation parameter of 0.1.
The default simulation values can be seen below:

<<default_args>>=
args(simulateLongData)
@




<<simulate_data1>>=
# simulate longitudinal data
# createLongData() is a wrapper for simulateLongData()
p <- list(nsubj = 20, beta0 = 1000, beta1 = 400, max.obs = 10, r.seed = 101,
          sd.pars = list(sigma = 250), auto.cor = 0.1)

ResponseData <- createLongData(subject=p)
head(ResponseData)
table(ResponseData$pid)
@

<<plot_long_pid, fig.width=10, fig.height=9>>=
# plot longitudinal traces by subject ("pid")
ResponseData %>%
  ggplot(aes(x = time, y = yij, group = pid, colour = pid)) +
  geom_point( size = 4, shape = 21, fill = "white") + 
  geom_line() + ylab(y_lab) + 
  guides(colour=guide_legend(ncol = 1)) +
  facet_wrap( "pid" ) + ggtitle(bquote(y[ij]~"~"~time))
@
<<plot_long_combined, fig.width=8, fig.height=7>>=
# plot longitudinal traces together
ResponseData %>%
  ggplot(aes(x = time, y = yij, group = pid, colour = pid)) +
  geom_point( size=4, shape = 21, fill = "white") + 
  geom_line() + ylab(y_lab) +
  guides(colour=guide_legend(ncol = 1)) +
  facet_wrap(vars(Group)) + ggtitle(bquote(y[ij]~"~"~time))
@


\subsection{Analysis in R}
The model we wish to fit:
%
\begin{equation}
   y_{ij} = b_{0i} + \beta_1 time_{ij} + \epsilon_{ij},
\end{equation}
%
again for the $j^{th}$ observation of the $i^{th}$ subject, where $b_{0i}$
is the subject-specific random \emph{intercept}, and $\beta_1$ is the
fixed-effect for \emph{time}.

<<fit1>>=
fit1 <- fit_lme_safely(yij ~ time, random=~1 | pid, data = ResponseData)
summary(fit1)
@

From the fixed-effects column in \code{Value}, the estimates are 
reasonably close to the original parameters:
$\hat{\beta_0}=\Sexpr{round(summary(fit1)[["tTable"]]["(Intercept)", "Value"], 2)}$ and 
$\hat{\beta_1}=\Sexpr{round(summary(fit1)[["tTable"]]["time", "Value"], 2)}$.
Both the slope and intercepts differ significantly from zero.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Example 2: Group Dependent Response (Interaction)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In clinical studies, it is often of interest to understand how the 
treatment of one group of subjects differs from another group of 
subjects (e.g. a control group). In a mixed-model setting, this is
accomplished by adding an interaction term to the model, which sets
up a conditional response variable, given that a subject belongs to
a particular group/class.

Next, simulate longitudinal data for 20 subjects, having $3-10$ serial
samples each, intercepts of $\beta_0=1000$, slopes of $\beta_1=0$ (crtl)
and $\beta_1=500$ (treat), and a serial autocorrelation parameter of 0.1.
The parameters that differ from the default simulation values can be seen below:

<<simulate_data2>>=
# simulate longitudinal data with group-specific response
control <- list(nsubj = 20, beta1 = 0, max.obs = 10, r.seed = 10,
                sd.pars = list(sigma = 150), auto.cor=0.1)
treatment <- list(nsubj = 20, beta1 = 500, max.obs = 10, r.seed = 11,
                  sd.pars = list(sigma = 350), auto.cor = 0.1)
GroupResponseData <- createLongData(control, treatment)
@

Now plot the longitudinal time traces:

<<plot_long_combined2, fig.width=12, fig.height=6>>=
GroupResponseData %>%
  ggplot(aes(x = time, y = yij, group = pid, colour = pid)) +
  geom_point(size = 4, shape = 21, fill = "white") + 
  geom_line() + ylab(y_lab) + 
  guides(colour = guide_legend(ncol = 2)) +
  facet_wrap(vars(Group)) + ggtitle(bquote(y[ij]~"~"~time))
@


%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis in R}
%%%%%%%%%%%%%%%%%%%%%%%%%%
The model specification to fit:
%
\begin{equation}
   y_{ij} = b_{0i} + \beta_1 time_{ij} + \beta_2 group_i + \beta_3 (group_i \times time_{ij}) + \epsilon_{ij},
\end{equation}
%
again for the $j^{th}$ observation of the $i^{th}$ subject,
$b_{0i}$ is still the subject-specific random intercept and $\beta_1$ is
still the fixed-effect for \emph{time}. However, now there is a
fixed-effect for \emph{group} and an interaction term ($\beta_3$ term)
that is multiplied by a dummy variable such that:

\begin{equation*}
\rowcolors{1}{}{}
   G = \begin{cases}
      0, & control \\ 
      1, & treatment
      \end{cases}
\end{equation*}
%
giving,
%
\begin{equation} \label{eqn:mixed_group}
   y_{ij} = b_{0i} + \beta_1 time_{ij} + \beta_2 group_i + \beta_3 (group_i \times time_{ij}) * G + \epsilon_{ij},
\end{equation}
%

<<fit2>>=
fit2 <- fit_lme_safely(yij ~ time * Group, random = ~1 | pid, data = GroupResponseData)
@


\subsection{Check Subject-specific linear models}
It is often a good idea to check the variation on the coefficients
of subject-specific linear fits of the data.
The \code{SomaMixedEffects::lmeDiagnostic()} function
creates a list of linear models for each subject ($i$). 

<<lmeDiagnostic, fig.width=10, fig.height=6>>=
# summarize the estimates and significance for each subject_i
lmeDiagnostic(fit2)
@

Notice that the subject-subject offsets ($\beta_0$) are approximately
centered about 1000 (RFU) for both treatment and control (as they
should be; we set them to the same intercept), and a vertical line up
from 1000 would cross the confidence interval for most subjects. The more
variable the estimates (and intervals) of $\beta_0$ seen in the left
panel, the greater the improvement in model fit can be found in fitting
subject-specific offsets in a mixed-effects model framework. Secondly, as
can be expected, the slope coefficient ($\beta_1$) is vastly different
between treated and control. Depending on the statistical question,
subject-specific slopes may be desired (but typically not for longitudinal
time-series data in a clinical setting).


\subsection{Summary}
<<coef2, echo=FALSE, results=FALSE>>=
tmp <- summary(fit2)$tTable
b0 <- round(tmp["(Intercept)","Value"],2)
b1 <- round(tmp["time","Value"],2)
b2 <- round(tmp["Grouptreatment","Value"],2)
b3 <- round(tmp["time:Grouptreatment","Value"],2)
t1 <- round(tmp["time:Grouptreatment","t-value"],2)
@

<<summary_fit2>>=
summary(fit2)
@

From the fixed-effects column in \code{Value}, the estimates are reasonably
close to the original parameters: $\hat{\beta_0}=\Sexpr{b0}$ and
$\hat{\beta_1}=\Sexpr{b1}$ (essentially zero). This indicates that the 
effect of time has no effect on $y_{ij}$ (at least for the controls!).
The significant \textbf{interaction} tells a different story and indicates
that the \textbf{entire} fixed-effect for \emph{time} occurs in the
\emph{treatment} group ($\hat{\beta_3}=\Sexpr{b3}$); in this case the
\emph{control} group does not vary with time.




%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Example 3}
%%%%%%%%%%%%%%%%%%%%%%%%
To illustrate how the model output is constructed, we simulate identically to the above,
however give the control group a slope of $\beta_1=250$. This results in a 
significant fixed-effect for time \emph{and} an significant interaction; the
additional effect of being in the \emph{treatment} group. Relate the values 
back to the original Eqn~\eqref{eqn:mixed_group} to see how the terms are partitioned.

<<plot_long_combined3, fig.width=12, fig.height=6>>=
# simulate longitudinal data with group-specific response
p1 <- list(nsubj = 20, beta1 = 250, max.obs = 10, r.seed = 10,
           sd.pars = list(sigma = 150), auto.cor = 0.1)
p2 <- list(nsubj = 20, beta1 = 500, max.obs = 10, r.seed = 101,
           sd.pars=list(sigma = 350), auto.cor = 0.1)
GroupResponseData2 <- createLongData(control = p1, treatment = p2)

GroupResponseData2 %>%
  ggplot(aes(x = time, y = yij, group = pid, colour = pid)) +
  geom_point( size = 4, shape = 21, fill = "white") + 
  geom_line() + ylab(y_lab) + 
  guides(colour = guide_legend(ncol = 2)) +
  facet_wrap(vars(Group)) + ggtitle(bquote(y[ij]~"~"~time))
@

<<fit3>>=
fit3 <- fit_lme_safely(yij ~ time * Group, random = ~1 | pid, data = GroupResponseData2)
summary(fit3)
@

<<coef3, echo=FALSE, results=FALSE>>=
tmp <- summary(fit3)$tTable
b0  <- round(tmp["(Intercept)", "Value"], 2)
b1  <- round(tmp["time", "Value"], 2)
b2  <- round(tmp["Grouptreatment", "Value"], 2)
b3  <- round(tmp["time:Grouptreatment", "Value"], 2)
t2  <- round(tmp["time:Grouptreatment", "t-value"], 2)
@

Notice that the $\hat{\beta_0}=\Sexpr{b0}$ has not changed,
as we haven't changed any baseline values, but now half of
the effect has moved into the \emph{time} effect
($\hat{\beta_1}=\Sexpr{b1}$), and away from the interaction 
coefficient ($\hat{\beta_3}=\Sexpr{b3}$) which was carrying
the entire slope effect for the treatment group.
Stated alternatively, the significance of the interaction
has dropped, though still a significant $p-$value, shifting
from \Sexpr{t1} to \Sexpr{t2}.



\newpage
%%%%%%%%%%%%%%%
\section{Notes}
%%%%%%%%%%%%%%%
\subsection{Mixed-effects Analysis Guidelines}
Below are some initial guidelines for the implementation of
linear mixed-effects modeling in {\sf R}.

\begin{enumerate}
  \item Make sure you are fitting the correct model for your
    desired question.
  \item Be sure you can write the full equation for the
    model(s) you are fitting.
  \item Prior to model fitting, plot the subject-specific coefficients
    with \code{nlme::lmList}.
  \item The \code{lme} function uses Expectation Maximization combined
    with Newton-Raphson iterations to fit the various model coefficients.
  \item A lme diagnostic wrapper can found in
    \code{SomaMixedEffects::lmeDiagnostic()}.
  \item A wrapper for \code{nlme::lme} to avoid convergence failures to
    stop your code from running to completion can be found in
    \code{SomaMixedEffects::fit\_lme\_safely()}.
  \item The functions below can be found in
    \code{SomaMixedEffects::fitMixedEffectsModels()} and
    \code{SomaMixedEffects::createMixedEffectsTable()}.
\end{enumerate}


\subsection{Sample Code For ADAT Analysis}
Univariate fitting of all analytes in an ADAT:

<<ADAT_example>>=
# see SomaMixedEffects/R/fitMixedEffectsModels.R
fitMixedEffectsModels 
@

\newpage
Combine the fitted models above into a statistical table:

<<ADAT_example2>>=
# see SomaMixedEffects/R/createMixedEffectsTable.R
createMixedEffectsTable 
@


\subsection{Pitfalls}
\begin{itemize}
 \item Mike Hinterberg: effects are fit sequentially according
   to the magnitude of the effect.  
 \item 
\end{itemize}

%\bibliographystyle{unsrt}
%\bibliography{main}
%\section*{Appendix}

\end{document}

